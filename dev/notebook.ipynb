{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install requests beautifulsoup4 lxml pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One - Extract informations\n",
    "#### Function below is necessary for scrape informations from Books to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from logger import get_logger\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import re\n",
    "import concurrent.futures\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "register = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _request_page(base_url: str, session, concat_url=None):\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    address = base_url + concat_url if concat_url else base_url\n",
    "    try:\n",
    "        r = session.get(url=address, timeout=5, headers=HEADERS)\n",
    "        if r.status_code == 200:\n",
    "            register.info(f\"Request OK, status code: {r.status_code}, URL: {address}\")\n",
    "            return r.content\n",
    "        else:\n",
    "            register.warning(f\"Bad Request: {r.status_code}, URL: {address}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        register.error(f\"Request failed: {e}, URL: {address}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_pages(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    address_founded = []\n",
    "    pattern = r\"page-(\\d+)\\.html\"\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"previous\" not in href and re.search(pattern, href):\n",
    "            href_formated = href.replace(\"catalogue/\", \"\")\n",
    "            if \"page-1.html\" in href_formated:\n",
    "                continue\n",
    "            else:\n",
    "                address_founded.append(href_formated)\n",
    "\n",
    "    return address_founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(base_url, session, sub_path=\"/catalogue/\"):\n",
    "    try:\n",
    "        visited_pages = set()\n",
    "        to_visit = [\"\"]\n",
    "        while to_visit:\n",
    "            current_page = to_visit.pop(0)\n",
    "            if current_page in visited_pages:\n",
    "                continue\n",
    "\n",
    "            url_base = base_url if current_page == \"\" else base_url + sub_path\n",
    "            register.info(f\"Current page is: {current_page}\")\n",
    "\n",
    "            content = _request_page(\n",
    "                base_url=url_base, session=session, concat_url=current_page\n",
    "            )\n",
    "            if content is None:\n",
    "                continue\n",
    "\n",
    "            visited_pages.add(current_page)\n",
    "\n",
    "            new_pages = _collect_pages(content=content)\n",
    "            for page in new_pages:\n",
    "                if page not in visited_pages and page not in to_visit:\n",
    "                    to_visit.append(page)\n",
    "\n",
    "        register.info(f\"Scraping completed. Pages visited: {len(visited_pages)}\")\n",
    "        return visited_pages\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'scrape_all_pages': {e}\")\n",
    "        if session:\n",
    "            register.info(\"Session close and finished.\")\n",
    "            session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_address_books(base_url, visited_pages, session):\n",
    "    try:\n",
    "        product_address = set()\n",
    "\n",
    "        for url in visited_pages:\n",
    "            content = _request_page(\n",
    "                base_url=base_url,\n",
    "                session=session,\n",
    "                concat_url=f\"/catalogue/{url}\" if url else None,\n",
    "            )\n",
    "            if content is None:\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            products_books = soup.find_all(class_=\"image_container\")\n",
    "\n",
    "            for product in products_books:\n",
    "                address = product.find(\"a\", href=True)\n",
    "                product_address.add(address.get(\"href\"))\n",
    "\n",
    "        register.info(\n",
    "            f\"Scraping completed. Books address collected: {len(product_address)}\"\n",
    "        )\n",
    "        return product_address\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in function 'scrape_all_address_books': {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'scrape_all_address_books': {e}\")\n",
    "        if session:\n",
    "            register.info(\"Session close and finished.\")\n",
    "            session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two - Parser and transform informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform_informations(book_title, price_of_book, qtd_stock, description_of_book):\n",
    "    if not book_title or not price_of_book or not qtd_stock or not description_of_book:\n",
    "        return \"N/A\", 0.0, 0, \"N/A\"\n",
    "\n",
    "    title = book_title.get_text(strip=True).capitalize() if book_title else \"N/A\"\n",
    "    price = (\n",
    "        float(price_of_book.get_text(strip=True).replace(\"Â£\", \"\"))\n",
    "        if price_of_book\n",
    "        else 0.0\n",
    "    )\n",
    "    stock = re.findall(r\"\\d+\", qtd_stock.get_text(strip=True)) if qtd_stock else []\n",
    "    stock = int(stock[0]) if stock else 0\n",
    "    description = description_of_book[0].strip() if description_of_book else \"N/A\"\n",
    "\n",
    "    return title, price, stock, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_books(base_url, product_address, session):\n",
    "    information = {\"title\": [], \"price\": [], \"stock\": [], \"description\": []}\n",
    "\n",
    "    def parse_url(url):\n",
    "        try:\n",
    "            url = url.replace(\"catalogue/\", \"\") if \"catalogue\" in url else url\n",
    "            content = _request_page(\n",
    "                base_url=base_url, session=session, concat_url=f\"/catalogue/{url}\"\n",
    "            )\n",
    "            if content is None:\n",
    "                return\n",
    "\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            tree = html.fromstring(content)\n",
    "\n",
    "            book_title = soup.find(\"h1\")\n",
    "            price_of_book = soup.find(class_=\"price_color\")\n",
    "            qtd_stock = soup.find(\"p\", class_=\"instock availability\")\n",
    "            description_of_book = tree.xpath(\n",
    "                '//*[@id=\"content_inner\"]/article/p/text()'\n",
    "            )\n",
    "\n",
    "            title, price, stock, description = _transform_informations(\n",
    "                book_title=book_title,\n",
    "                price_of_book=price_of_book,\n",
    "                qtd_stock=qtd_stock,\n",
    "                description_of_book=description_of_book,\n",
    "            )\n",
    "\n",
    "            if title == \"N/A\" or price == 0.0 or stock == 0 or description == \"N/A\":\n",
    "                register.warning(f\"Warning: Invalid data for URL {url}\")\n",
    "\n",
    "            information[\"title\"].append(title)\n",
    "            information[\"price\"].append(price)\n",
    "            information[\"stock\"].append(stock)\n",
    "            information[\"description\"].append(description)\n",
    "\n",
    "        except Exception as e:\n",
    "            register.error(f\"Exception in function 'parse_url' for URL {url}: {e}\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(parse_url, product_address)\n",
    "\n",
    "    return information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three - Connect to database where we increment informations from scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_to_database(database):\n",
    "    try:\n",
    "        con = sqlite3.connect(database=database)\n",
    "        cursor = con.cursor()\n",
    "        register.info(\"Connection to database OK!\")\n",
    "        return con, cursor\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'connection_to_database': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_for_books(cursor, con):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"\"\"CREATE TABLE IF NOT EXISTS books (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            title TEXT NOT NULL,\n",
    "            description TEXT,\n",
    "            price REAL NOT NULL,\n",
    "            availability INTEGER NOT NULL\n",
    "        );\"\"\"\n",
    "        )\n",
    "        con.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'insert_values': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_values(cursor, con, title, price, stock, description):\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"\"\"INSERT INTO books(title,description,price,availability) VALUES(?,?,?,?)\"\"\",\n",
    "            (title, description, price, stock),\n",
    "        )\n",
    "        con.commit()\n",
    "        register.info(f\"Insert values from book: {title} OK!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'insert_values': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_db_csv(database):\n",
    "    try:\n",
    "        con = sqlite3.connect(database)\n",
    "        query = \"\"\"SELECT * FROM books\"\"\"\n",
    "        df = pd.read_sql(query, con)\n",
    "        df.to_csv(\"registers.csv\", index=False)\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'output_db_csv': {e}\")\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://books.toscrape.com\"\n",
    "DATABASE = \"database.sqlite3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with requests.Session() as session:\n",
    "        visited_pages = scrape_all_pages(base_url=URL, session=session)\n",
    "        product_address = scrape_all_address_books(\n",
    "            base_url=URL, visited_pages=visited_pages, session=session\n",
    "        )\n",
    "        information = parser_books(base_url=URL, product_address=product_address, session=session)\n",
    "        num_books = len(information[\"title\"])\n",
    "        con, cursor = connection_to_database(database=DATABASE)\n",
    "        create_table_for_books(con=con, cursor=cursor)\n",
    "        for i in range(num_books):\n",
    "            title = information[\"title\"][i]\n",
    "            price = information[\"price\"][i]\n",
    "            stock = information[\"stock\"][i]\n",
    "            description = information[\"description\"][i]\n",
    "            insert_values(\n",
    "                cursor=cursor,\n",
    "                con=con,\n",
    "                title=title,\n",
    "                price=price,\n",
    "                stock=stock,\n",
    "                description=description,\n",
    "            )\n",
    "finally:\n",
    "    if con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_db_csv(database=DATABASE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
