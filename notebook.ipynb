{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from logger import get_logger\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "register = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _request_page(base_url: str, concat_url=None):\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    with requests.Session() as session:\n",
    "        address = base_url + concat_url if concat_url else base_url\n",
    "        try:\n",
    "            r = session.get(url=address, timeout=5, headers=HEADERS)\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                register.info(\n",
    "                    f\"Request OK, status code: {r.status_code}, URL: {address}\"\n",
    "                )\n",
    "                return r.content, session\n",
    "            else:\n",
    "                register.warning(f\"Bad Request: {r.status_code}, URL: {address}\")\n",
    "        except requests.RequestException as e:\n",
    "            register.error(f\"Request failed: {e}, URL: {address}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collect_pages(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    address_founded = []\n",
    "    pattern = r\"page-(\\d+)\\.html\"\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"previous\" not in href and re.search(pattern, href):\n",
    "            href_formated = href.replace(\"catalogue/\", \"\")\n",
    "            if \"page-1.html\" in href_formated:\n",
    "                continue\n",
    "            else:\n",
    "                address_founded.append(href_formated)\n",
    "\n",
    "    return address_founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(base_url, session=None, sub_path=\"/catalogue/\"):\n",
    "    try:\n",
    "\n",
    "        visited_pages = set()\n",
    "        to_visit = [\"\"]\n",
    "        while to_visit:\n",
    "            current_page = to_visit.pop(0)\n",
    "            if current_page in visited_pages:\n",
    "                continue\n",
    "\n",
    "            if current_page == \"\":\n",
    "                url_base = base_url\n",
    "            else:\n",
    "                url_base = base_url + sub_path\n",
    "                register.info(f\"Current page is: {current_page}\")\n",
    "\n",
    "            content, session = _request_page(base_url=url_base, concat_url=current_page)\n",
    "            if content is None:\n",
    "                continue\n",
    "\n",
    "            visited_pages.add(current_page)\n",
    "\n",
    "            new_pages = _collect_pages(content=content)\n",
    "            for page in new_pages:\n",
    "                if page not in visited_pages and page not in to_visit:\n",
    "                    to_visit.append(page)\n",
    "        register.info(f\"Scraping completed. Pages visited: {len(visited_pages)}\")\n",
    "        return visited_pages, session\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'scrape_all_pages': {e}\")\n",
    "        if session:\n",
    "            register.info(\"Session close and finished.\")\n",
    "            session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_address_books(base_url, visited_pages, session):\n",
    "    try:\n",
    "        for url in visited_pages:\n",
    "            if url == \"\":\n",
    "                content, session = _request_page(base_url=base_url)\n",
    "\n",
    "            elif url != \"\":\n",
    "                content, session = _request_page(\n",
    "                    base_url=base_url, concat_url=(\"catalogue/\" + url)\n",
    "                )\n",
    "\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            products_books = soup.find_all(class_=\"product_pod\")\n",
    "            product_address = set()\n",
    "\n",
    "            for product in products_books:\n",
    "                address = product.find(\"a\", href=True)\n",
    "                product_address.add(address.get(\"href\"))\n",
    "\n",
    "        register.info(\n",
    "            f\"Scraping completed. Books address collected: {len(visited_pages)}\"\n",
    "        )\n",
    "        return product_address, session\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'scrape_all_address_books': {e}\")\n",
    "        if session:\n",
    "            register.info(\"Session close and finished.\")\n",
    "            session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_books(base_url, product_address, session):\n",
    "    try:\n",
    "        print(len(product_address))\n",
    "        for url in product_address:\n",
    "            print(url)\n",
    "            content, session = _request_page(\n",
    "                base_url=base_url, concat_url=(\"catalogue/\" + url)\n",
    "            )\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            book_title = soup.find(\"h1\").text\n",
    "            price = soup.find(class_=\"price_color\").text\n",
    "            qtd_stock = soup.find(\"i\", class_=\"icon-ok\").text\n",
    "\n",
    "    except Exception as e:\n",
    "        register.error(f\"Exception in fuction 'parser_books': {e}\")\n",
    "        if session:\n",
    "            register.info(\"Session close and finished.\")\n",
    "            session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://books.toscrape.com/\"\n",
    "visited_pages, session = scrape_all_pages(base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_address, session = scrape_all_address_books(\n",
    "    base_url=base_url, visited_pages=visited_pages, session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_books(base_url=base_url, product_address=product_address, session=session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
